{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.  Instructions Gram-Schmidt process\n",
    "In this assignment you will write a function to perform the Gram-Schmidt procedure, which takes a list of vectors and forms an orthonormal basis from this set.\n",
    "As a corollary, the procedure allows us to determine the dimension of the space spanned by the basis vectors, which is equal to or less than the space which the vectors sit.\n",
    "\n",
    "### Matrices in Python\n",
    "Remember the structure for matrices in *numpy* is,\n",
    "```python\n",
    "A[0, 0]  A[0, 1]  A[0, 2]  A[0, 3]\n",
    "A[1, 0]  A[1, 1]  A[1, 2]  A[1, 3]\n",
    "A[2, 0]  A[2, 1]  A[2, 2]  A[2, 3]\n",
    "A[3, 0]  A[3, 1]  A[3, 2]  A[3, 3]\n",
    "```\n",
    "You can access the value of each element individually using,\n",
    "```python\n",
    "A[n, m]\n",
    "```\n",
    "You can also access a whole row at a time using,\n",
    "```python\n",
    "A[n]\n",
    "```\n",
    "\n",
    "You can select whole columns at a time.\n",
    "This can be done with,\n",
    "```python\n",
    "A[:, m]\n",
    "```\n",
    "which will select the m'th column (starting at zero).\n",
    "\n",
    "In this exercise, you will need to take the dot product between vectors. This can be done using the @ operator.\n",
    "To dot product vectors u and v, use the code,\n",
    "```python\n",
    "u @ v\n",
    "```\n",
    "\n",
    "All the code you should complete will be at the same level of indentation as the instruction comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verySmallNumber = 1e-14 # That's 1×10⁻¹⁴ = 0.00000000000001\n",
    "\n",
    "\n",
    "def gsBasis(A) :\n",
    "    B = np.array(A, dtype=np.float_) # Make B as a copy of A, since we're going to alter it's values.\n",
    "    # Loop over all vectors, starting with zero, label them with i\n",
    "    for i in range(B.shape[1]):\n",
    "        # Inside that loop, loop over all previous vectors, j, to subtract.\n",
    "        for j in range(i) :\n",
    "            # Complete the code to subtract the overlap with previous vectors.\n",
    "            # you'll need the current vector B[:, i] and a previous vector B[:, j]\n",
    "            e_j = B[:, j]\n",
    "            proj = (np.dot(B[:, i],e_j)*e_j)\n",
    "            B[:, i] = B[:, i] - proj    ## EDIT THIS\n",
    "        # Next insert code to do the normalisation test for B[:, i]\n",
    "        if la.norm(B[:, i]) > verySmallNumber:\n",
    "            B[:, i] = B[:, i]/la.norm(B[:, i]) ## EDIT THIS\n",
    "        else :\n",
    "            B[:, i] = np.zeros_like(B[:, i])      \n",
    "            \n",
    "    # Finally, we return the result:\n",
    "    return B\n",
    "\n",
    "# This function uses the Gram-schmidt process to calculate the dimension\n",
    "# spanned by a list of vectors.\n",
    "# Since each vector is normalised to one, or is zero,\n",
    "# the sum of all the norms will be the dimension.\n",
    "def dimensions(A) :\n",
    "    return np.sum(la.norm(gsBasis(A), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using numpy implementation, \n",
    "\n",
    "def gs(X):\n",
    "    Q, R = np.linalg.qr(X) # mode='complete')\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([[1,0,2,6],\n",
    "              [0,1,8,2],\n",
    "              [2,8,3,1],\n",
    "              [1,-6,2,3]], dtype=np.float_)\n",
    "\n",
    "\n",
    "M = np.array([[1,1,1],[1,1,0], [1,0,0]], dtype=np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsBasis(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40824829,  0.1814885 ,  0.04982278, -0.89325973],\n",
       "       [-0.        , -0.1088931 ,  0.99349591,  0.03328918],\n",
       "       [-0.81649658, -0.50816781, -0.06462163,  0.26631346],\n",
       "       [-0.40824829,  0.83484711,  0.07942048,  0.36063281]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we have a big problem here, the python implemntation differs from \n",
    "## our implementation. Bonus point if you can figure out why.\n",
    "# There is a sign difference but I don't know why, I thin they are using different implementation\n",
    "#np.testing.assert_almost_equal(gs(V), gsBasis(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23643312,  0.18771349,  0.22132104],\n",
       "       [ 0.15762208,  0.74769023, -0.64395812],\n",
       "       [ 0.15762208,  0.57790444,  0.72904263],\n",
       "       [ 0.94573249, -0.26786082, -0.06951101]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what happens for non-square matrices\n",
    "A = np.array([[3,2,3],\n",
    "              [2,5,-1],\n",
    "              [2,4,8],\n",
    "              [12,2,1]], dtype=np.float_)\n",
    "gsBasis(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23643312, -0.18771349, -0.22132104],\n",
       "       [-0.15762208, -0.74769023,  0.64395812],\n",
       "       [-0.15762208, -0.57790444, -0.72904263],\n",
       "       [-0.94573249,  0.26786082,  0.06951101]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93704257, -0.12700832, -0.32530002,  0.        ,  0.        ],\n",
       "       [ 0.31234752,  0.72140727,  0.61807005,  0.        ,  0.        ],\n",
       "       [ 0.15617376, -0.6807646 ,  0.71566005,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[6,2,1,7,5],\n",
    "              [2,8,5,-4,1],\n",
    "              [1,-6,3,2,8]], dtype=np.float_)\n",
    "gsBasis(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.70710678, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see what happens when we have one vector that is a linear combination of the others.\n",
    "C = np.array([[1,0,2],\n",
    "              [0,1,-3],\n",
    "              [1,0,2]], dtype=np.float_)\n",
    "gsBasis(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Describe the PageRank Algorithm as seen in the python notebook on Page_rank.ipynb and relate it with the use of eigne-values and eigen-vectors. \n",
    "\n",
    "Provide your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "PageRank is an analysis algorithm that assigns a numerical weighting to each element of a set of web pages, with the purpose of \"measuring\" its relative importance within the set. PageRank works by counting the number and quality of links to a web page to determine a rough estimate of how important the web page is. The underlying assumption is that more important websites are likely to receive more links from other websites.\n",
    "\n",
    "The rank of the page is a result of an iterative process based on the webgraph, created by all World Wide Web pages as nodes and hyperlinks as edges. A hyperlink to a page counts as a vote of support. The PageRank of a page is depends on the number and PageRank metric of all pages that link to it i.e \"incoming links\". A page that is linked to by many pages with high PageRank receives a high rank itself.\n",
    "\n",
    "### The algorithm\n",
    "- The algorithm starts by creating a sparse square \"web graph\" $M$ matrix where position $(i.j)$ is set to value $1$ if there is a link between page $i$ and page $j$. If one page is a sink i.e it it has no outgoing links it will be assumed that it links out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web.\n",
    "\n",
    "\n",
    "- Each column $j$ in the matrix represent the number of outgoing links from page j. To make sure all links has the same weight all columns are then normalized.\n",
    "\n",
    "\n",
    "- The PageRank theory holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue is a damping factor $d$. Various studies have tested different damping factors, but it is generally assumed that the damping factor will be set around 0.85.\n",
    "\n",
    "\n",
    "- The vector $v$ is randomly initialized and is updated iteratively using the following algorithm until convergence. \n",
    "\n",
    "$\\hat{M} = d * M + \\frac{(1-d)}{number of pages} * Ones$\n",
    " \n",
    "repeat until convergence:\n",
    "\n",
    "$v_{new} = \\hat{M} * v_{old}$\n",
    "\n",
    "where $M$ is the web graph matrix with it's columns normalized.\n",
    "\n",
    "- The PageRank values $v$ after convergence are the entries of the eigenvector of the modified matrix $\\hat{M} $rescaled so that each column adds up to one. i.e $v$ can be seen as the eigen values matrix for the transformation matrix $\\hat{M}$ with eigen value 1."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "linear-algebra-machine-learning",
   "graded_item_id": "FNk8v",
   "launcher_item_id": "DdvVk"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
